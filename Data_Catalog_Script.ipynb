{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Catalog Python Script\n",
    "* Repository located here: https://github.com/emilyporter920/Data_Cataloging"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import dependencies\n",
    "from snowflake.snowpark.session import Session\n",
    "import json\n",
    "import pandas as pd\n",
    "from platform import python_version\n",
    "from datetime import datetime\n",
    "import openpyxl\n",
    "from config import account, user, authenticator, warehouse1, role1, warehouse2, role2 \n",
    "\n",
    "# Shows Python version (SnowPark uses anything below 3.8.x)\n",
    "print(python_version())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GVR PROD Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Snowflake Session object (GVR_PROD)\n",
    "connection_parameters = {\n",
    "    \"account\": account,\n",
    "    \"user\": user,\n",
    "    \"authenticator\": authenticator,\n",
    "    \"warehouse\": warehouse1,\n",
    "    \"role\": role1\n",
    "}\n",
    "\n",
    "session = Session.builder.configs(connection_parameters).create()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Primary Keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecting the UAT data from the Snowflake table\n",
    "primary_keys = session.sql(\"SHOW PRIMARY KEYS IN DATABASE GVR_PROD\").collect()\n",
    "\n",
    "primary_keys = pd.DataFrame(list(primary_keys))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get rid of columns you don't need in PRIMARY_KEYS table\n",
    "primary_keys = primary_keys.drop(['created_on', 'constraint_name', 'rely'], axis=1)\n",
    "primary_keys.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename columns in PRIMARY_KEYS table to match COLUMN_TABLE columns\n",
    "primary_keys = primary_keys.rename(columns= {'database_name': 'DATABASE', 'schema_name': 'SCHEMA', \n",
    "                                             'table_name': 'TABLE_NAME', 'column_name': 'COLUMN_NAME', 'key_sequence': 'PK'})\n",
    "\n",
    "primary_keys.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Information Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Information_Schema data\n",
    "column_table = session.sql(\"SELECT TABLE_CATALOG, TABLE_SCHEMA, TABLE_NAME, COLUMN_NAME, DATA_TYPE, CHARACTER_MAXIMUM_LENGTH, NUMERIC_PRECISION, NUMERIC_SCALE, IS_NULLABLE FROM GVR_PROD.INFORMATION_SCHEMA.COLUMNS\").to_pandas()\n",
    "\n",
    "column_table.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename columns in COLUMN_TABLE table\n",
    "column_table = column_table.rename(columns= {'TABLE_CATALOG': 'DATABASE', 'TABLE_SCHEMA': 'SCHEMA', 'CHARACTER_MAXIMUM_LENGTH': 'LENGTH', \n",
    "                                             'NUMERIC_PRECISION': 'PRECISION', 'NUMERIC_SCALE': 'SCALE', 'IS_NULLABLE': 'NULLABLE'})\n",
    "\n",
    "column_table.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge PRIMARY_KEYS and COLUMN_TABLE tables\n",
    "merged_tables = pd.merge(column_table, primary_keys, how='left', on=['DATABASE', 'SCHEMA', 'TABLE_NAME', 'COLUMN_NAME'])\n",
    "\n",
    "merged_tables.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get rid of schemas: Information_Schema, Admin, and Stage\n",
    "merged_tables=merged_tables[~merged_tables['SCHEMA'].isin(['INFORMATION_SCHEMA'])]\n",
    "\n",
    "merged_tables.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IS360 Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Snowflake Session object (IS360)\n",
    "connection_parameters = {\n",
    "    \"account\": account,\n",
    "    \"user\": user,\n",
    "    \"authenticator\": authenticator,\n",
    "    \"warehouse\": warehouse2,\n",
    "    \"role\": role2\n",
    "}\n",
    "\n",
    "session = Session.builder.configs(connection_parameters).create()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Primary Keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecting the UAT data from the Snowflake table\n",
    "primary_keys2 = session.sql(\"SHOW PRIMARY KEYS IN DATABASE GVR_IS360_DEV_DB\").collect()\n",
    "\n",
    "primary_keys2 = pd.DataFrame(list(primary_keys2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get rid of columns you don't need in PRIMARY_KEYS table\n",
    "primary_keys2 = primary_keys2.drop(['created_on', 'constraint_name', 'rely'], axis=1)\n",
    "\n",
    "primary_keys2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename columns in PRIMARY_KEYS table to match COLUMN_TABLE columns\n",
    "primary_keys2 = primary_keys2.rename(columns= {'database_name': 'DATABASE', 'schema_name': 'SCHEMA', \n",
    "                                             'table_name': 'TABLE_NAME', 'column_name': 'COLUMN_NAME', 'key_sequence': 'PK'})\n",
    "\n",
    "primary_keys2.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Information Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Information_Schema data\n",
    "column_table2 = session.sql(\"SELECT TABLE_CATALOG, TABLE_SCHEMA, TABLE_NAME, COLUMN_NAME, DATA_TYPE, CHARACTER_MAXIMUM_LENGTH, NUMERIC_PRECISION, NUMERIC_SCALE, IS_NULLABLE FROM GVR_IS360_DEV_DB.INFORMATION_SCHEMA.COLUMNS\").to_pandas()\n",
    "\n",
    "column_table2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename columns in COLUMN_TABLE table\n",
    "column_table2 = column_table2.rename(columns= {'TABLE_CATALOG': 'DATABASE', 'TABLE_SCHEMA': 'SCHEMA', 'CHARACTER_MAXIMUM_LENGTH': 'LENGTH', \n",
    "                                             'NUMERIC_PRECISION': 'PRECISION', 'NUMERIC_SCALE': 'SCALE', 'IS_NULLABLE': 'NULLABLE'})\n",
    "\n",
    "column_table2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge PRIMARY_KEYS and COLUMN_TABLE tables\n",
    "merged_tables2 = pd.merge(column_table2, primary_keys2, how='left', on=['DATABASE', 'SCHEMA', 'TABLE_NAME', 'COLUMN_NAME'])\n",
    "\n",
    "merged_tables2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get rid of schemas: Information_Schema, Admin, and Stage\n",
    "merged_tables3 = merged_tables2[~merged_tables2['SCHEMA'].isin(['INFORMATION_SCHEMA', 'STAGE', 'ADMIN'])]\n",
    "\n",
    "merged_tables3.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merge GVR PROD with IS360 Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Appending IS360 to GVR_PROD dataframe\n",
    "merged_tables3 = merged_tables.append(merged_tables2, ignore_index=True)\n",
    "\n",
    "merged_tables3.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generates values for the ZONE column\n",
    "def zone(column):\n",
    "    if column['SCHEMA'] in ['SMS', 'SALESFORCE', 'AX', 'MAC-PAC', 'PROTHEUS_AR', 'PROTHEUS_BR', 'PROTHEUS_CH', 'QAD', 'HFM', 'INSITE360_TELEMETRY', 'AVA_DEMO', 'IOT_CORE', 'AVA_LEGACY', \n",
    "                            'AVA_DEV', 'AVA_CORE_DEV', 'AVA_UAT', 'AVA_QA', 'AVA_CORE_UAT', 'AVA', 'ARCHIVE', 'AVA_CORE_QA', 'AVA_CORE_DEMO', 'PUSH_SALE_EVENT', 'CENSUS']:\n",
    "        val = 'DATALAKE'\n",
    "    elif column['SCHEMA'] in ['DATA_MART_FIN_NA', 'DATA_MART_AMO_NA', 'DATA_MART_FIN_GLOBAL', 'DATA_MART_CUSTOMER']:\n",
    "        val = 'DATAMART'\n",
    "    elif column['SCHEMA'] in ['DW']:\n",
    "        val = 'DATAWAREHOUSE'\n",
    "    elif column['SCHEMA'] in ['RPT']:\n",
    "        val = 'CONSUMPTION'\n",
    "    else:\n",
    "        val = ' '\n",
    "    return val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the zone function to the merged_tables3 dataframe\n",
    "merged_tables3['ZONE'] = merged_tables3.apply(zone, axis=1)\n",
    "\n",
    "merged_tables3.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Historical Retention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the historical retention data (hard coded for now, will be available in the future when this historical retention changes)\n",
    "merged_tables3['HISTORICAL_RETENTION'] = '32 DAYS'\n",
    "\n",
    "merged_tables3.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Times"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GVR PROD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load_table created to show when the table was last loaded\n",
    "load_table1 = session.sql(\"SELECT SCHEMA_NAME, TABLE_NAME, LAST_LOAD_TIME FROM GVR_PROD.INFORMATION_SCHEMA.LOAD_HISTORY\").to_pandas()\n",
    "\n",
    "load_table1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change the last_load_time to only be yyyy/mm/dd\n",
    "load_table1['LAST_LOAD_TIME'] = pd.to_datetime(load_table1['LAST_LOAD_TIME']).dt.date\n",
    "\n",
    "load_table1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename columns in LOAD_TABLE table to match MERGED_TABLES table\n",
    "load_table1 = load_table1.rename(columns= {'SCHEMA_NAME': 'SCHEMA'})\n",
    "\n",
    "load_table1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort values to show the most recent load times\n",
    "most_recent_times1 = load_table1.sort_values('LAST_LOAD_TIME', ascending=False)\n",
    "\n",
    "most_recent_times1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select only the most recent load time\n",
    "most_recent_times1 = most_recent_times1.drop_duplicates(subset='TABLE_NAME', keep='first')\n",
    "\n",
    "most_recent_times1 = most_recent_times1.reset_index(drop=True)\n",
    "\n",
    "most_recent_times1.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IS360"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load_table created to show when the table was last loaded\n",
    "load_table2 = session.sql(\"SELECT SCHEMA_NAME, TABLE_NAME, LAST_LOAD_TIME FROM GVR_IS360_DEV_DB.INFORMATION_SCHEMA.LOAD_HISTORY\").to_pandas()\n",
    "\n",
    "load_table2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change the last_load_time to only be yyyy/mm/dd\n",
    "load_table2['LAST_LOAD_TIME'] = pd.to_datetime(load_table2['LAST_LOAD_TIME']).dt.date\n",
    "\n",
    "load_table2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename columns in LOAD_TABLE table to match MERGED_TABLES table\n",
    "load_table2 = load_table2.rename(columns= {'SCHEMA_NAME': 'SCHEMA'})\n",
    "\n",
    "load_table2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort values to show the most recent load times\n",
    "most_recent_times2 = load_table2.sort_values('LAST_LOAD_TIME', ascending=False)\n",
    "\n",
    "most_recent_times2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select only the most recent load time \n",
    "most_recent_times2 = most_recent_times2.drop_duplicates(subset='TABLE_NAME', keep='first')\n",
    "\n",
    "most_recent_times2 = most_recent_times2.reset_index(drop=True)\n",
    "\n",
    "most_recent_times2.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appending GVR PROD & IS360 Load Times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Append the two most recent times tables together\n",
    "most_recent_times = most_recent_times1.append(most_recent_times2, ignore_index=True)\n",
    "\n",
    "most_recent_times.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge Load Times To Merged_Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge PRIMARY_KEYS and COLUMN_TABLE tables\n",
    "merged_tables3 = pd.merge(merged_tables3, most_recent_times, how='left', on=['SCHEMA', 'TABLE_NAME'])\n",
    "\n",
    "merged_tables3.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table Type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hard code that all table types are Type1 (History Available)\n",
    "merged_tables3['TABLE_TYPE'] = 'History Available'\n",
    "\n",
    "merged_tables3.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dropping Schemas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get rid of schemas: Information_Schema, Admin, and Stage\n",
    "merged_tables3 = merged_tables3[~merged_tables3['SCHEMA'].isin(['INFORMATION_SCHEMA', 'STAGE', 'ADMIN'])]\n",
    "\n",
    "merged_tables3.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify the columns are all capitalized\n",
    "merged_tables3.columns = merged_tables3.columns.str.upper()\n",
    "\n",
    "merged_tables3.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rearrange Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rearrange columns - to be added later"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Write to Excel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pd.ExcelWriter('Catalog/Data_Catalog.xlsx', mode='a', engine='openpyxl', if_sheet_exists=\"replace\",) as writer:\n",
    "    merged_tables3.to_excel(writer, sheet_name='Catalog', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "snowparkenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
